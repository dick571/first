ä»¥ä¸‹æ˜¯åŸºäº PyTorch å®ç°çš„ Transformer ç¼–ç å™¨ï¼ˆEncoderï¼‰ çš„å®Œæ•´ä»£ç ç¤ºä¾‹ï¼ŒåŒ…å« å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Self-Attentionï¼‰ã€ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ã€å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Networkï¼‰ ä»¥åŠ æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰å’Œå±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰ ç­‰æ ¸å¿ƒç»„ä»¶ï¼Œå¹¶é™„æœ‰è¯¦ç»†æ³¨é‡Šå’Œè¯´æ˜ã€‚
ğŸ“Œ ä¸€ã€Transformer ç¼–ç å™¨ç»“æ„æ¦‚è¿°
Transformer ç¼–ç å™¨ç”±ä»¥ä¸‹ç»„ä»¶æ„æˆï¼š

è¾“å…¥åµŒå…¥ï¼ˆInput Embeddingï¼‰ï¼šå°†è¾“å…¥åºåˆ—è½¬æ¢ä¸ºè¯å‘é‡ã€‚
ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ï¼šä¸ºæ¨¡å‹æä¾›åºåˆ—ä¸­è¯è¯­çš„ä½ç½®ä¿¡æ¯ã€‚
N ä¸ªç›¸åŒçš„ç¼–ç å™¨å±‚ï¼ˆEncoder Layerï¼‰ï¼š
å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Self-Attentionï¼‰
å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Networkï¼‰
æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
ğŸ“Œ äºŒã€å®Œæ•´ä»£ç å®ç°

import torch
import torch.nn as nn
import math

# 1. å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Self-Attentionï¼‰
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super(MultiHeadAttention, self).__init__()
        self.n_heads = n_heads
        self.d_model = d_model
        self.head_dim = d_model // n_heads

        assert self.head_dim * n_heads == d_model, "d_model must be divisible by n_heads"

        # çº¿æ€§å˜æ¢ï¼šç”Ÿæˆ Q, K, V
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)

        # è¾“å‡ºçº¿æ€§å±‚
        self.fc_out = nn.Linear(d_model, d_model)

        # ç¼©æ”¾å› å­
        self.scale = 1.0 / (self.head_dim ** 0.5)

    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)

        # 1. çº¿æ€§å˜æ¢
        Q = self.w_q(query)  # (batch_size, seq_len, d_model)
        K = self.w_k(key)    # (batch_size, seq_len, d_model)
        V = self.w_v(value)  # (batch_size, seq_len, d_model)

        # 2. åˆ†å‰²ä¸ºå¤šä¸ªå¤´
        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)

        # 3. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-1, -2)) * self.scale  # (batch_size, n_heads, seq_len, seq_len)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention = torch.softmax(scores, dim=-1)

        # 4. åŠ æƒæ±‚å’Œ
        out = torch.matmul(attention, V)  # (batch_size, n_heads, seq_len, head_dim)

        # 5. åˆå¹¶å¤šå¤´
        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)

        # 6. è¾“å‡ºçº¿æ€§å˜æ¢
        return self.fc_out(out)

# 2. å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆPosition-wise Feed-Forward Networkï¼‰
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.fc2(self.dropout(torch.relu(self.fc1(x))))

# 3. ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

# 4. ç¼–ç å™¨å±‚ï¼ˆEncoder Layerï¼‰
class EncoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super(EncoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention(d_model, n_heads)
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, src, src_mask=None):
        # 1. å¤šå¤´è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å½’ä¸€åŒ–
        src2 = self.self_attn(src, src, src, src_mask)
        src = self.norm1(src + self.dropout1(src2))

        # 2. å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å½’ä¸€åŒ–
        src2 = self.ffn(src)
        src = self.norm2(src + self.dropout2(src2))

        return src

# 5. ç¼–ç å™¨ï¼ˆEncoderï¼‰
class Encoder(nn.Module):
    def __init__(self, src_vocab_size, d_model, n_layers, n_heads, d_ff, dropout=0.1, max_len=5000):
        super(Encoder, self).__init__()
        self.embedding = nn.Embedding(src_vocab_size, d_model)
        self.positional_encoding = PositionalEncoding(d_model, max_len)
        self.layers = nn.ModuleList([
            EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)
        ])
        self.dropout = nn.Dropout(dropout)

    def forward(self, src, src_mask=None):
        # 1. è¾“å…¥åµŒå…¥ + ä½ç½®ç¼–ç 
        src = self.embedding(src)
        src = self.positional_encoding(src)
        src = self.dropout(src)

        # 2. å¤šå±‚ç¼–ç å™¨å¤„ç†
        for layer in self.layers:
            src = layer(src, src_mask)

        return src

ğŸ“Œ ä¸‰ã€æµ‹è¯•ä»£ç ä¸è¾“å‡º
# ç¤ºä¾‹å‚æ•°
src_vocab_size = 10000  # æºè¯­è¨€è¯æ±‡è¡¨å¤§å°
d_model = 512           # è¯åµŒå…¥ç»´åº¦
n_layers = 6            # ç¼–ç å™¨å±‚æ•°
n_heads = 8             # æ³¨æ„åŠ›å¤´æ•°
d_ff = 2048             # å‰é¦ˆç½‘ç»œä¸­é—´å±‚ç»´åº¦
dropout = 0.1           # Dropoutæ¯”ç‡
max_len = 100           # æœ€å¤§åºåˆ—é•¿åº¦

# åˆ›å»ºç¼–ç å™¨
encoder = Encoder(src_vocab_size, d_model, n_layers, n_heads, d_ff, dropout, max_len)

# æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
batch_size = 32
seq_len = 20
src = torch.randint(0, src_vocab_size, (batch_size, seq_len))  # (batch_size, seq_len)

# å‰å‘ä¼ æ’­
output = encoder(src)
print("ç¼–ç å™¨è¾“å‡ºå½¢çŠ¶:", output.shape)  # åº”è¾“å‡º (32, 20, 512)

ğŸ“Œ å››ã€ä»£ç è§£æä¸å…³é”®æ¦‚å¿µ
ç»„ä»¶	åŠŸèƒ½è¯´æ˜
MultiHeadAttention	å®ç°å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ•æ‰åºåˆ—å†…éƒ¨ä¸åŒè¯­ä¹‰å…³ç³»ã€‚
PositionwiseFeedForward	ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œï¼Œç”¨äºå¼•å…¥éçº¿æ€§å˜æ¢ã€‚
PositionalEncoding	é€šè¿‡æ­£å¼¦/ä½™å¼¦å‡½æ•°ç”Ÿæˆä½ç½®ç¼–ç ï¼Œä½¿æ¨¡å‹æ„ŸçŸ¥åºåˆ—é¡ºåºã€‚
EncoderLayer	åŒ…å«å¤šå¤´æ³¨æ„åŠ›ã€å‰é¦ˆç½‘ç»œã€æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–ã€‚
Encoder	å¤šä¸ªç¼–ç å™¨å±‚å †å ï¼Œå®Œæˆæ•´ä¸ªç¼–ç è¿‡ç¨‹ã€‚

ğŸ“Œ äº”ã€æ³¨æ„äº‹é¡¹ä¸æ‰©å±•å»ºè®®
é¡¹ç›®	è¯´æ˜
ä½ç½®ç¼–ç æ–¹å¼	å¯é€‰æ­£å¼¦å‡½æ•°æˆ–å¯å­¦ä¹ çš„ nn.Embeddingï¼Œæ ¹æ®ä»»åŠ¡é€‰æ‹©ã€‚
æ©ç æœºåˆ¶	åœ¨ç¼–ç å™¨ä¸­é€šå¸¸ä½¿ç”¨å¡«å……æ©ç ï¼ˆpadding maskï¼‰ï¼Œé˜²æ­¢æ— æ•ˆä½ç½®å‚ä¸è®¡ç®—ã€‚
å¤šå¤´æ³¨æ„åŠ›ä¼˜åŒ–	ä½¿ç”¨ einsum æˆ– nn.MultiheadAttention å¯æå‡æ•ˆç‡ã€‚
æ‰©å±•åº”ç”¨	å¯ç”¨äºæœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ã€å›¾åƒåˆ†ç±»ï¼ˆç»“åˆ Vision Transformerï¼‰ã€‚

âœ… æ€»ç»“
Transformer ç¼–ç å™¨ æ˜¯ NLP ä»»åŠ¡çš„æ ¸å¿ƒæ¨¡å—ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ•æ‰åºåˆ—å†…éƒ¨çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚
æœ¬å®ç°åŸºäº PyTorchï¼ŒåŒ…å«å¤šå¤´è‡ªæ³¨æ„åŠ›ã€å‰é¦ˆç½‘ç»œã€æ®‹å·®è¿æ¥ã€å±‚å½’ä¸€åŒ–ç­‰å…³é”®ç»„ä»¶ã€‚
ä»£ç ç»“æ„æ¸…æ™°ï¼Œä¾¿äºæ‰©å±•ä¸ºå®Œæ•´çš„ Transformer æ¨¡å‹ï¼ˆç»“åˆè§£ç å™¨ï¼‰æˆ–åº”ç”¨äº Vision Transformerã€‚
