ä»¥ä¸‹æ˜¯ä¸€ä¸ªåŸºäº PyTorch çš„ è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰ å®ç°ä»£ç ï¼ŒåŒ…å« å•å¤´è‡ªæ³¨æ„åŠ› å’Œ å¤šå¤´è‡ªæ³¨æ„åŠ› çš„å®Œæ•´å®ç°ï¼Œå¹¶é™„æœ‰è¯¦ç»†æ³¨é‡Šå’Œè§£é‡Šã€‚
ğŸ“Œ ä¸€ã€å•å¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSingle-Head Self-Attentionï¼‰
import torch
import torch.nn as nn
import torch.nn.functional as F

class SelfAttention(nn.Module):
    def __init__(self, embed_size):
        super(SelfAttention, self).__init__()
        self.embed_size = embed_size  # è¾“å…¥ç‰¹å¾çš„ç»´åº¦

        # çº¿æ€§å˜æ¢å±‚ï¼šç”Ÿæˆ Query, Key, Value
        self.query = nn.Linear(embed_size, embed_size, bias=False)
        self.key = nn.Linear(embed_size, embed_size, bias=False)
        self.value = nn.Linear(embed_size, embed_size, bias=False)

        # ç¼©æ”¾å› å­ï¼ˆé˜²æ­¢ç‚¹ç§¯è¿‡å¤§ï¼‰
        self.scale = 1.0 / (embed_size ** 0.5)

    def forward(self, x):
        """
        è¾“å…¥ x çš„å½¢çŠ¶: (batch_size, seq_len, embed_size)
        è¾“å‡ºçš„å½¢çŠ¶: (batch_size, seq_len, embed_size)
        """
        # 1. ç”Ÿæˆ Query, Key, Value
        Q = self.query(x)  # (batch_size, seq_len, embed_size)
        K = self.key(x)    # (batch_size, seq_len, embed_size)
        V = self.value(x)  # (batch_size, seq_len, embed_size)

        # 2. è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç‚¹ç§¯ï¼‰
        # Q: (batch_size, seq_len, embed_size)
        # K.T: (batch_size, embed_size, seq_len)
        # attention_scores: (batch_size, seq_len, seq_len)
        attention_scores = torch.matmul(Q, K.transpose(-1, -2)) * self.scale

        # 3. åº”ç”¨ Softmax å½’ä¸€åŒ–
        attention_weights = F.softmax(attention_scores, dim=-1)  # (batch_size, seq_len, seq_len)

        # 4. åŠ æƒæ±‚å’Œ Value
        # attention_weights: (batch_size, seq_len, seq_len)
        # V: (batch_size, seq_len, embed_size)
        # out: (batch_size, seq_len, embed_size)
        out = torch.matmul(attention_weights, V)

        return out


ğŸ“Œ äºŒã€å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Self-Attentionï¼‰
class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_size, num_heads):
        super(MultiHeadSelfAttention, self).__init__()
        self.embed_size = embed_size
        self.num_heads = num_heads
        self.head_dim = embed_size // num_heads

        assert self.head_dim * num_heads == embed_size, "Embed size must be divisible by num_heads"

        # çº¿æ€§å˜æ¢å±‚ï¼šç”Ÿæˆ Query, Key, Value
        self.query = nn.Linear(embed_size, embed_size, bias=False)
        self.key = nn.Linear(embed_size, embed_size, bias=False)
        self.value = nn.Linear(embed_size, embed_size, bias=False)

        # è¾“å‡ºæŠ•å½±å±‚
        self.fc_out = nn.Linear(embed_size, embed_size)

        # ç¼©æ”¾å› å­
        self.scale = 1.0 / (self.head_dim ** 0.5)

    def forward(self, x):
        """
        è¾“å…¥ x çš„å½¢çŠ¶: (batch_size, seq_len, embed_size)
        è¾“å‡ºçš„å½¢çŠ¶: (batch_size, seq_len, embed_size)
        """
        batch_size, seq_len, _ = x.size()

        # 1. ç”Ÿæˆ Query, Key, Value
        Q = self.query(x)  # (batch_size, seq_len, embed_size)
        K = self.key(x)    # (batch_size, seq_len, embed_size)
        V = self.value(x)  # (batch_size, seq_len, embed_size)

        # 2. åˆ†å‰²ä¸ºå¤šä¸ªå¤´ï¼ˆnum_headsï¼‰
        # Q: (batch_size, num_heads, seq_len, head_dim)
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # 3. è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç‚¹ç§¯ï¼‰
        # Q: (batch_size, num_heads, seq_len, head_dim)
        # K: (batch_size, num_heads, seq_len, head_dim)
        # attention_scores: (batch_size, num_heads, seq_len, seq_len)
        attention_scores = torch.matmul(Q, K.transpose(-1, -2)) * self.scale

        # 4. åº”ç”¨ Softmax å½’ä¸€åŒ–
        attention_weights = F.softmax(attention_scores, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)

        # 5. åŠ æƒæ±‚å’Œ Value
        # attention_weights: (batch_size, num_heads, seq_len, seq_len)
        # V: (batch_size, num_heads, seq_len, head_dim)
        # out: (batch_size, num_heads, seq_len, head_dim)
        out = torch.matmul(attention_weights, V)

        # 6. åˆå¹¶å¤šå¤´è¾“å‡º
        # out: (batch_size, seq_len, embed_size)
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_size)

        # 7. è¾“å‡ºæŠ•å½±
        out = self.fc_out(out)

        return out

ğŸ“Œ å››ã€ä»£ç è§£æä¸å…³é”®æ¦‚å¿µ
1. è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³
Query, Key, Valueï¼šé€šè¿‡çº¿æ€§å˜æ¢ç”Ÿæˆï¼Œåˆ†åˆ«è¡¨ç¤ºå½“å‰è¯çš„æŸ¥è¯¢å‘é‡ã€å…¶ä»–è¯çš„é”®å‘é‡å’Œå€¼å‘é‡ã€‚
ç›¸ä¼¼åº¦è®¡ç®—ï¼šä½¿ç”¨ç‚¹ç§¯è¡¡é‡ Query å’Œ Key ä¹‹é—´çš„ç›¸å…³æ€§ã€‚
Softmax å½’ä¸€åŒ–ï¼šå°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºæ³¨æ„åŠ›æƒé‡ã€‚
åŠ æƒæ±‚å’Œï¼šæ ¹æ®æ³¨æ„åŠ›æƒé‡å¯¹ Value è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡ºã€‚
2. å¤šå¤´è‡ªæ³¨æ„åŠ›çš„ä¼˜åŠ¿
å¤šè§†è§’å»ºæ¨¡ï¼šå°†è¾“å…¥åˆ†å‰²æˆå¤šä¸ªå¤´ï¼Œæ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›ï¼Œæ•æ‰ä¸åŒè¯­ä¹‰å…³ç³»ã€‚
å¢å¼ºè¡¨è¾¾èƒ½åŠ›ï¼šå¤šå¤´è¾“å‡ºåˆå¹¶åï¼Œèƒ½æ›´å¥½åœ°æ•æ‰å¤æ‚ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
å¹¶è¡Œè®¡ç®—ï¼šå¤šå¤´ä¹‹é—´å¯ä»¥å¹¶è¡Œè®¡ç®—ï¼Œæå‡æ•ˆç‡ã€‚
3. å…³é”®å…¬å¼
æ³¨æ„åŠ›æƒé‡è®¡ç®—ï¼š

å¤šå¤´æ³¨æ„åŠ›ï¼š


ğŸ“Œ äº”ã€æ³¨æ„äº‹é¡¹ä¸æ‰©å±•
é¡¹ç›®	è¯´æ˜
ç¼©æ”¾å› å­	é˜²æ­¢ç‚¹ç§¯å€¼è¿‡å¤§å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼Œé€šå¸¸ä½¿ç”¨  ç¼©æ”¾ã€‚
ä½ç½®ç¼–ç 	è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸è€ƒè™‘é¡ºåºï¼Œéœ€é¢å¤–åŠ å…¥ä½ç½®ç¼–ç ï¼ˆå¦‚æ­£å¼¦/ä½™å¼¦å‡½æ•°æˆ–å¯å­¦ä¹ åµŒå…¥ï¼‰ã€‚
æ©ç æœºåˆ¶	åœ¨è§£ç å™¨ä¸­ï¼Œéœ€å¼•å…¥æ©ç é˜²æ­¢æœªæ¥ä¿¡æ¯æ³„éœ²ï¼ˆmasked attentionï¼‰ã€‚
ä¼˜åŒ–å»ºè®®	ä½¿ç”¨ einsum æˆ– torch.matmul å®ç°é«˜æ•ˆçŸ©é˜µè¿ç®—ã€‚
æ‰©å±•åº”ç”¨	è‡ªæ³¨æ„åŠ›æœºåˆ¶å¹¿æ³›ç”¨äº NLPï¼ˆå¦‚ Transformerï¼‰ã€å›¾åƒå¤„ç†ï¼ˆå¦‚ Vision Transformerï¼‰ç­‰é¢†åŸŸã€‚
