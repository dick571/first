ä»¥ä¸‹æ˜¯ä¸€ä¸ªåŸºäº scikit-learn åº“å®ç°çš„ K-Means èšç±»ç®—æ³• çš„å®Œæ•´ä»£ç ç¤ºä¾‹ï¼Œæ¶µç›– æ•°æ®ç”Ÿæˆã€æ ‡å‡†åŒ–ã€è‚˜éƒ¨æ³•åˆ™é€‰æ‹©æœ€ä½³Kå€¼ã€æ¨¡å‹è®­ç»ƒä¸å¯è§†åŒ– ç­‰å®Œæ•´æµç¨‹ï¼Œå¹¶é™„æœ‰è¯¦ç»†æ³¨é‡Šå’Œè§£é‡Šã€‚

ğŸ“Œ ä¸€ã€K-Means èšç±»ä»£ç å®ç°ï¼ˆä½¿ç”¨ scikit-learnï¼‰
# 1. å¯¼å…¥å¿…è¦çš„åº“
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# 2. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
# ç”Ÿæˆä¸€ä¸ªäºŒç»´æ•°æ®é›†ï¼ŒåŒ…å«3ä¸ªç°‡
X, y = make_blobs(n_samples=300, centers=3, cluster_std=0.6, random_state=42)

# å¯è§†åŒ–åŸå§‹æ•°æ®
plt.scatter(X[:, 0], X[:, 1], s=50, c='blue', label='Original Data')
plt.title('Original Data Distribution')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.show()

# 3. æ•°æ®æ ‡å‡†åŒ–ï¼ˆK-Meanså¯¹ç‰¹å¾å°ºåº¦æ•æ„Ÿï¼‰
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™ï¼ˆElbow Methodï¼‰é€‰æ‹©æœ€ä½³Kå€¼
wcss = []  # å­˜å‚¨ä¸åŒKå€¼çš„è¯¯å·®å¹³æ–¹å’Œï¼ˆSSEï¼‰
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', n_init=10, random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)  # inertia_ æ˜¯æ¨¡å‹çš„SSE

# ç»˜åˆ¶è‚˜éƒ¨æ›²çº¿
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, 'bo-', linewidth=2, markersize=8)
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Sum of Squared Errors (SSE)')
plt.axvline(x=3, color='r', linestyle='--', label='Elbow Point (K=3)')
plt.legend()
plt.grid(True)
plt.show()

# 5. ä½¿ç”¨æœ€ä½³Kå€¼ï¼ˆK=3ï¼‰è¿›è¡Œèšç±»
kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, random_state=42)
kmeans.fit(X_scaled)

# è·å–èšç±»ç»“æœ
y_kmeans = kmeans.predict(X_scaled)
centers = kmeans.cluster_centers_

# 6. å¯è§†åŒ–èšç±»ç»“æœ
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_kmeans, s=50, cmap='viridis', label='Clustered Data')
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, marker='X', label='Centroids')
plt.title('K-Means Clustering Result (K=3)')
plt.xlabel('Standardized Feature 1')
plt.ylabel('Standardized Feature 2')
plt.legend()
plt.show()

# 7. æ¨¡å‹è¯„ä¼°ï¼ˆè½®å»“ç³»æ•°ï¼‰
score = silhouette_score(X_scaled, y_kmeans)
print(f"Silhouette Score (K=3): {score:.4f}")

äºŒã€ä»£ç è§£æä¸å…³é”®æ¦‚å¿µ
1. æ•°æ®ç”Ÿæˆä¸å¯è§†åŒ–
ä½¿ç”¨ make_blobs ç”ŸæˆäºŒç»´æ¨¡æ‹Ÿæ•°æ®ï¼ŒåŒ…å«3ä¸ªç°‡ï¼Œä¾¿äºå¯è§†åŒ–ã€‚
åŸå§‹æ•°æ®æ•£ç‚¹å›¾å¸®åŠ©ç†è§£æ•°æ®åˆ†å¸ƒã€‚
2. æ•°æ®æ ‡å‡†åŒ–
K-Means å¯¹ç‰¹å¾å°ºåº¦æ•æ„Ÿï¼Œä½¿ç”¨ StandardScaler è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ã€‚
3. è‚˜éƒ¨æ³•åˆ™ï¼ˆElbow Methodï¼‰
é€šè¿‡è®¡ç®—ä¸åŒKå€¼ä¸‹çš„ è¯¯å·®å¹³æ–¹å’Œï¼ˆSSEï¼‰ï¼Œç»˜åˆ¶æ›²çº¿å¯»æ‰¾â€œæ‹ç‚¹â€ã€‚
æ‹ç‚¹å¤„çš„Kå€¼å³ä¸ºæœ€ä½³èšç±»æ•°ï¼ˆæœ¬ä¾‹ä¸­ä¸º3ï¼‰ã€‚
4. K-Means æ¨¡å‹è®­ç»ƒ
ä½¿ç”¨ KMeans ç±»ï¼Œè®¾ç½® n_clusters=3ï¼Œinit='k-means++' é¿å…ä¸­å¿ƒç‚¹åˆå§‹åŒ–ä¸å½“ã€‚
n_init=10 è¡¨ç¤ºè¿è¡Œ10æ¬¡ä¸åŒåˆå§‹ä¸­å¿ƒçš„èšç±»ï¼Œå–æœ€ä¼˜ç»“æœã€‚
5. èšç±»ç»“æœå¯è§†åŒ–
ä¸åŒé¢œè‰²è¡¨ç¤ºä¸åŒç°‡ï¼Œçº¢è‰²æ˜Ÿå·è¡¨ç¤ºèšç±»ä¸­å¿ƒã€‚
æ ‡å‡†åŒ–åçš„ç‰¹å¾ä½¿å¾—èšç±»æ›´å‡†ç¡®ã€‚
6. æ¨¡å‹è¯„ä¼°
ä½¿ç”¨ è½®å»“ç³»æ•°ï¼ˆSilhouette Scoreï¼‰ è¯„ä¼°èšç±»æ•ˆæœï¼Œå€¼è¶Šæ¥è¿‘1è¡¨ç¤ºèšç±»æ•ˆæœè¶Šå¥½

ä¸‰ã€æ³¨æ„äº‹é¡¹ä¸è¿›é˜¶å»ºè®®
é¡¹ç›®	è¯´æ˜
Kå€¼é€‰æ‹©	è‚˜éƒ¨æ³•åˆ™å’Œè½®å»“ç³»æ•°ç»“åˆä½¿ç”¨æ›´å¯é ï¼Œå®é™…åº”ç”¨ä¸­éœ€ç»“åˆä¸šåŠ¡éœ€æ±‚ã€‚
åˆå§‹åŒ–æ–¹å¼	æ¨èä½¿ç”¨ k-means++ åˆå§‹åŒ–ï¼Œé¿å…éšæœºåˆå§‹åŒ–å¯¼è‡´å±€éƒ¨æœ€ä¼˜ã€‚
ç‰¹å¾ç»´åº¦	é«˜ç»´æ•°æ®éœ€é™ç»´ï¼ˆå¦‚PCAï¼‰åå¯è§†åŒ–ï¼Œæˆ–ä½¿ç”¨å…¶ä»–è¯„ä¼°æŒ‡æ ‡ã€‚
å¼‚å¸¸å€¼å¤„ç†	K-Means å¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼Œå»ºè®®å…ˆè¿›è¡Œæ•°æ®æ¸…æ´—æˆ–ä½¿ç”¨DBSCANç­‰é²æ£’ç®—æ³•ã€‚
æ‰©å±•åº”ç”¨	å¯ç”¨äºå®¢æˆ·åˆ†ç¾¤ã€å›¾åƒå‹ç¼©ã€å¼‚å¸¸æ£€æµ‹ç­‰åœºæ™¯ã€‚

å››ã€æ‰‹åŠ¨å®ç°K-Meansï¼ˆä»…ç”¨äºç†è§£åŸç†ï¼‰
def kmeans_manual(X, k, max_iter=100, tol=1e-4):
    # 1. åˆå§‹åŒ–ä¸­å¿ƒç‚¹ï¼ˆéšæœºé€‰æ‹©ï¼‰
    centers = X[np.random.choice(X.shape[0], size=k, replace=False)]
    
    for _ in range(max_iter):
        # 2. åˆ†é…æ ·æœ¬åˆ°æœ€è¿‘ç°‡
        distances = np.sqrt(((X - centers[:, np.newaxis])**2).sum(axis=2))  # è®¡ç®—è·ç¦»
        labels = np.argmin(distances, axis=0)  # æ‰¾åˆ°æœ€è¿‘çš„ç°‡
        
        # 3. æ›´æ–°ä¸­å¿ƒç‚¹
        new_centers = np.array([X[labels == i].mean(axis=0) for i in range(k)])
        
        # 4. åˆ¤æ–­æ˜¯å¦æ”¶æ•›
        if np.all(np.abs(new_centers - centers) < tol):
            break
        
        centers = new_centers
    
    return labels, centers

# ä½¿ç”¨æ‰‹åŠ¨å®ç°çš„K-Means
labels, centers = kmeans_manual(X_scaled, k=3)

# å¯è§†åŒ–ç»“æœ
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, s=50, cmap='viridis')
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, marker='X')
plt.title('Manual K-Means Clustering')
plt.xlabel('Standardized Feature 1')
plt.ylabel('Standardized Feature 2')
plt.show()
